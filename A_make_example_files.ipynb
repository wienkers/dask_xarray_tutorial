{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Example Data\n",
    "_cf. EERIE Cycle 1 Spin-up_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import intake\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8d8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/scratch/b/b382615/dask_example_scratch/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local Cluster\n",
    "cluster = LocalCluster(n_workers=64, threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "\n",
    "remote_node = subprocess.run(['hostname'], capture_output=True, text=True).stdout.strip().split('.')[0]\n",
    "port = re.search(r':(\\d+)/', client.dashboard_link).group(1)\n",
    "print(f\"Forward with Port = {remote_node}:{port}\")\n",
    "\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_catalog(\"https://raw.githubusercontent.com/eerie-project/intake_catalogues/main/eerie.yaml\")\n",
    "expid = 'eerie-control-1950'\n",
    "version = 'v20231106'\n",
    "model = 'icon-esm-er'\n",
    "gridspec = 'native'\n",
    "\n",
    "dat = cat['dkrz.disk.model-output'][model][expid][version]['ocean'][gridspec]\n",
    "\n",
    "ds = dat['2d_daily_mean'](chunks={'time':2,'ncells':-1}).to_dask().sel(time=slice('2010-01-01','2040-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5685fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fast NN Interpolator\n",
    "#    NB: this is not acceptable from a conservation or accuracy point of view, but it's great to make a lot of data ;)\n",
    "\n",
    "points_native = np.vstack((ds.lon, ds.lat)).T\n",
    "\n",
    "# Make 0.1 degree grid\n",
    "n_lat = 1800\n",
    "n_lon = 3600\n",
    "lons = np.linspace(-180, 180, n_lon+1); lons = lons[:-1]\n",
    "lats = np.linspace(-90, 90, n_lat+1); lats = lats[:-1]\n",
    "lon_mesh, lat_mesh = np.meshgrid(lons, lats)\n",
    "points_regrid = np.vstack((lon_mesh.flatten(), lat_mesh.flatten())).T\n",
    "\n",
    "tree = cKDTree(points_native)\n",
    "_, nn_indices = tree.query(points_regrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1276c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mesh_shape = lat_mesh.shape\n",
    "nn_indices_da = xr.DataArray(nn_indices, dims=('nn_index'))\n",
    "def nn_native_regrid_parallel(var_native, nn_indices):\n",
    "    values_regrid = var_native[nn_indices].reshape(lat_mesh_shape)\n",
    "    return values_regrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a25bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid(da):\n",
    "    regridded = xr.apply_ufunc(nn_native_regrid_parallel, \n",
    "                                da, nn_indices_da,\n",
    "                                input_core_dims=[['ncells'],['nn_index']],\n",
    "                                output_core_dims=[['lat','lon']],\n",
    "                                output_sizes={'lat': n_lat, 'lon': n_lon}, \n",
    "                                output_dtypes=(np.float32),\n",
    "                                vectorize=True,\n",
    "                                dask='parallelized')\n",
    "    regridded = regridded.assign_coords(lat=('lat', lats), lon=('lon', lons))\n",
    "    return regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1630d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_x = regrid(ds.atmos_fluxes_stress_xw)\n",
    "u = regrid(ds.u.isel(depth=0))\n",
    "sst = regrid(ds.to.isel(depth=0))\n",
    "ds_new = xr.Dataset({'tau_x': tau_x, 'u': u, 'sst': sst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474677ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ae9ab",
   "metadata": {},
   "source": [
    "## Save to Disk:\n",
    "1. zarr: Chunksize time = 1, space = -1\n",
    "2. zarr: Chunksize time = 20, space = 5x\n",
    "3. Series of netcdf files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90668f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1:\n",
    "\n",
    "ds_new = ds_new.chunk({'time':1, 'lat': -1, 'lon': -1})\n",
    "encoding = {var: {'compressor': None} for var in ds_new.data_vars}\n",
    "ds_new.to_zarr(data_dir+'/example_data_chunks1.zarr', mode='w', encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2:\n",
    "\n",
    "ds_new = xr.open_zarr(data_dir+'/example_data_chunks1.zarr', chunks={})\n",
    "# ds_new.to_netcdf(data_dir+'/example_data.nc', mode='w')\n",
    "\n",
    "# Specify the output file path and name\n",
    "output_path = data_dir + '/example_mfdataset/'\n",
    "output_name = \"output_\"\n",
    "\n",
    "# Get the unique years in the dataset\n",
    "years = pd.to_datetime(ds_new.time.values).year.unique()\n",
    "\n",
    "# Create a list to store the chunked datasets\n",
    "chunked_datasets = []\n",
    "\n",
    "# Iterate over the years\n",
    "for year in years:\n",
    "    # Select the data for the current year\n",
    "    year_data = ds_new.sel(time=str(year))\n",
    "    \n",
    "    # Append the year data to the list\n",
    "    chunked_datasets.append(year_data)\n",
    "\n",
    "# Save the chunked datasets using save_mfdataset\n",
    "xr.save_mfdataset(\n",
    "    datasets=chunked_datasets,\n",
    "    paths=[output_path + output_name + f\"{year}.nc\" for year in years],\n",
    "    mode=\"w\",\n",
    "    engine=\"netcdf4\",\n",
    "    format=\"NETCDF4\",\n",
    "    unlimited_dims=[\"time\"],\n",
    "    compute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9387496",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new = xr.open_zarr(data_dir+'/example_data_chunks1.zarr', chunks={'time':20, 'lat': 36, 'lon': -1})\n",
    "encoding = {var: {'compressor': None} for var in ds_new.data_vars}\n",
    "ds_new.to_zarr(data_dir+'/example_data_chunks2.zarr', mode='w', encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new = xr.open_zarr(data_dir+'/example_data_chunks2.zarr', chunks={'time':100, 'lat': 4, 'lon': -1})\n",
    "encoding = {var: {'compressor': None} for var in ds_new.data_vars}\n",
    "ds_new.to_zarr(data_dir+'/example_data_chunks3.zarr', mode='w', encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
